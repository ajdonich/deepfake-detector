{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, random, time\n",
    "import psycopg2, json\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# For managing relative imports from notebook\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import config.config as dfc\n",
    "import deepfake.dfutillib as df\n",
    "import deepfake.postgresdb as pgdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 1334 valid videos, parse time: 0.153 sec\n",
      "Partition 1: 1699 valid videos, parse time: 0.163 sec\n",
      "Partition 2: 1748 valid videos, parse time: 0.159 sec\n",
      "Partition 3: 1455 valid videos, parse time: 0.135 sec\n",
      "Partition 4: 1701 valid videos, parse time: 0.163 sec\n",
      "Partition 5: 2483 valid videos, parse time: 0.209 sec\n",
      "Partition 6: 3464 valid videos, parse time: 0.297 sec\n",
      "Partition 7: 2473 valid videos, parse time: 0.242 sec\n",
      "Partition 8: 1816 valid videos, parse time: 0.163 sec\n",
      "Partition 9: 1736 valid videos, parse time: 0.149 sec\n",
      "Partition 10: 3192 valid videos, parse time: 0.277 sec\n",
      "Partition 11: 2118 valid videos, parse time: 0.184 sec\n",
      "Partition 12: 2225 valid videos, parse time: 0.187 sec\n",
      "Partition 13: 3694 valid videos, parse time: 0.302 sec\n",
      "Partition 14: 2464 valid videos, parse time: 0.206 sec\n",
      "Partition 15: 2273 valid videos, parse time: 0.188 sec\n",
      "Partition 16: 2061 valid videos, parse time: 0.176 sec\n",
      "Partition 17: 2430 valid videos, parse time: 0.202 sec\n",
      "Partition 18: 2683 valid videos, parse time: 0.223 sec\n",
      "Partition 19: 2752 valid videos, parse time: 0.229 sec\n",
      "Partition 20: 2154 valid videos, parse time: 0.180 sec\n",
      "Partition 21: 2268 valid videos, parse time: 0.226 sec\n",
      "Partition 22: 2409 valid videos, parse time: 0.193 sec\n",
      "Partition 23: 2410 valid videos, parse time: 0.198 sec\n",
      "Partition 24: 2786 valid videos, parse time: 0.238 sec\n",
      "Partition 25: 2546 valid videos, parse time: 0.223 sec\n",
      "Partition 26: 2433 valid videos, parse time: 0.219 sec\n",
      "Partition 27: 2353 valid videos, parse time: 0.206 sec\n",
      "Partition 28: 2085 valid videos, parse time: 0.179 sec\n",
      "Partition 29: 2557 valid videos, parse time: 0.210 sec\n",
      "Partition 30: 2236 valid videos, parse time: 0.177 sec\n",
      "Partition 31: 2470 valid videos, parse time: 0.211 sec\n",
      "Partition 32: 2356 valid videos, parse time: 0.197 sec\n",
      "Partition 33: 2274 valid videos, parse time: 0.187 sec\n",
      "Partition 34: 2658 valid videos, parse time: 0.268 sec\n",
      "Partition 35: 2535 valid videos, parse time: 0.237 sec\n",
      "Partition 36: 2339 valid videos, parse time: 0.228 sec\n",
      "Partition 37: 2655 valid videos, parse time: 0.254 sec\n",
      "Partition 38: 2477 valid videos, parse time: 0.232 sec\n",
      "Partition 39: 2556 valid videos, parse time: 0.228 sec\n",
      "Partition 40: 2420 valid videos, parse time: 0.206 sec\n",
      "Partition 41: 2222 valid videos, parse time: 0.188 sec\n",
      "Partition 42: 2384 valid videos, parse time: 0.201 sec\n",
      "Partition 43: 2546 valid videos, parse time: 0.238 sec\n",
      "Partition 44: 2665 valid videos, parse time: 0.254 sec\n",
      "Partition 45: 2346 valid videos, parse time: 0.196 sec\n",
      "Partition 46: 2202 valid videos, parse time: 0.192 sec\n",
      "Partition 47: 2406 valid videos, parse time: 0.216 sec\n",
      "Partition 48: 2463 valid videos, parse time: 0.208 sec\n",
      "Partition 49: 3134 valid videos, parse time: 0.283 sec\n",
      "\n",
      "Total dataset:\n",
      "  107232 training videos\n",
      "  11914 validation videos\n",
      "\n",
      "Assigning epoch video-blocks...\n",
      "  595 epoch video-blocks assigned\n",
      "\n",
      "Remaining back-fill videos:\n",
      "  132 training videos\n",
      "  14 validation videos\n",
      "\n",
      "Attempting database insert of:\n",
      "  107100 training videos\n",
      "  11900 validation videos\n",
      "  146 back-fill videos\n",
      "\n",
      "WARNING: table 'epoch_queue' already exists with 0 rows.\n",
      "WARNING: table 'videos' already exists with 119146 rows.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Are you sure you want reinitialize this database?\n",
      "[N/y] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Commencing database initialization...\n",
      "Database initialization complete.\n",
      "\n",
      "Commencing database population...\n",
      "Database population complete.\n",
      "\n",
      "CPU times: user 6.85 s, sys: 6.47 s, total: 13.3 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function reads each partition's metadata.json file, compiles a correpsonding \n",
    "# list of all videos, split into order-randomized training and validation sets,\n",
    "# then assigns these to epoch blocks and inserts everything into the database.\n",
    "\n",
    "def create_videos_data(istart, istop=None, validation_split=0.1, epochsz=200):\n",
    "#{\n",
    "    # Like range args\n",
    "    if istop is None: istart, istop = 0, istart\n",
    "\n",
    "    vtrains, vvalids = [],[]\n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        vpart, initial = [], time.time()\n",
    "        try:\n",
    "            # Store all valid tuples from the partition metadata file\n",
    "            with open(f\"{df.traindir(i)}/metadata.json\") as jsonfile:\n",
    "            #{\n",
    "                metadata = json.load(jsonfile)\n",
    "                for vidname, meta in metadata.items():\n",
    "                #{\n",
    "                    if df.file_exists(f\"{df.traindir(i)}/{vidname}\"):\n",
    "                        vtup = pgdb.VideoTuple(vidname=vidname, partition=i, label=meta['label'])\n",
    "                        if meta['label'] == 'REAL': vpart.append(vtup) \n",
    "                        elif df.file_exists(f\"{df.traindir(i)}/{meta['original']}\"):\n",
    "                            vtup.origname = meta['original']\n",
    "                            vpart.append(vtup)\n",
    "                #}\n",
    "            #}\n",
    "        except PermissionError as err: print(\"ERROR:\", err)\n",
    "\n",
    "        # Randomly select a validation subset\n",
    "        nvalids = round(validation_split*len(vpart))\n",
    "        vindices = set(random.sample(range(len(vpart)), nvalids))\n",
    "\n",
    "        # Separate into respective split\n",
    "        for j in range(len(vpart)):\n",
    "        #{\n",
    "            if j in vindices:\n",
    "                vpart[j].split = \"validate\"\n",
    "                vvalids.append(vpart[j])\n",
    "            else:\n",
    "                vpart[j].split = \"train\"\n",
    "                vtrains.append(vpart[j])\n",
    "        #}\n",
    "        \n",
    "        print((f\"Partition {i}: {len(vpart)} valid videos, \"\n",
    "               f\"parse time: {time.time()-initial:.3f} sec\"))\n",
    "    #}\n",
    "\n",
    "    print(f\"\\nTotal dataset:\")\n",
    "    print(f\"  {len(vtrains)} training videos\")\n",
    "    print(f\"  {len(vvalids)} validation videos\")\n",
    "    print(\"\\nAssigning epoch video-blocks...\")\n",
    "    \n",
    "    # Randomize video orders\n",
    "    np.random.shuffle(vtrains)\n",
    "    np.random.shuffle(vvalids)\n",
    "\n",
    "    # Init step sizes and create epoch blocks\n",
    "    nvalids = int(epochsz*validation_split)    \n",
    "    rgt = range(0, len(vtrains), epochsz - nvalids)\n",
    "    rgv = range(0, len(vvalids), nvalids)\n",
    "    \n",
    "    for blkid, (ti, vi) in enumerate(zip(rgt, rgv)):\n",
    "        for vtup in vtrains[ti:ti+epochsz-nvalids]: vtup.blk_id = blkid\n",
    "        for vtup in vvalids[vi:vi+nvalids]: vtup.blk_id = blkid\n",
    "    print(f\"  {blkid} epoch video-blocks assigned\")\n",
    "\n",
    "    # Save straggler remainders to back-fill unreadable video\n",
    "    print(f\"\\nRemaining back-fill videos:\")\n",
    "    print(f\"  {len(vtrains[ti:])} training videos\")\n",
    "    print(f\"  {len(vvalids[vi:])} validation videos\")\n",
    "    \n",
    "    for vtup in vtrains[ti:]: vtup.blk_id = -1\n",
    "    for vtup in vvalids[vi:]: vtup.blk_id = -1\n",
    "    \n",
    "    # Insert epoch video-blocks into DB\n",
    "    print(f\"\\nAttempting database insert of:\")\n",
    "    print(f\"  {len(vtrains[:ti])} training videos\")\n",
    "    print(f\"  {len(vvalids[:vi])} validation videos\")\n",
    "    print(f\"  {len(vtrains[ti:])+len(vvalids[vi:])} back-fill videos\\n\")\n",
    "        \n",
    "    with pgdb.PostgreSqlHandle() as db_handle:\n",
    "        if not db_handle.initialize_database(): print(\"Populate aborted\")\n",
    "        else: db_handle.populate_database(vtrains, vvalids)\n",
    "#}\n",
    "               \n",
    "\n",
    "create_videos_data(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
