{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, random\n",
    "import psycopg2, json\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# For managing relative imports from notebook\n",
    "if '..' not in sys.path: sys.path.append('..')\n",
    "\n",
    "import config.config as dfc\n",
    "import deepfake.dfutillib as df\n",
    "import deepfake.postgresdb as pgdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 1334 videos\n",
      "Partition 1: 1699 videos\n",
      "Partition 2: 1748 videos\n",
      "Partition 3: 1455 videos\n",
      "Partition 4: 1701 videos\n",
      "Partition 5: 2483 videos\n",
      "Partition 6: 3464 videos\n",
      "Partition 7: 2473 videos\n",
      "Partition 8: 1816 videos\n",
      "Partition 9: 1736 videos\n",
      "Partition 10: 3192 videos\n",
      "Partition 11: 2118 videos\n",
      "Partition 12: 2225 videos\n",
      "Partition 13: 3694 videos\n",
      "Partition 14: 2464 videos\n",
      "Partition 15: 2273 videos\n",
      "Partition 16: 2061 videos\n",
      "Partition 17: 2430 videos\n",
      "Partition 18: 2683 videos\n",
      "Partition 19: 2752 videos\n",
      "Partition 20: 2154 videos\n",
      "Partition 21: 2268 videos\n",
      "Partition 22: 2409 videos\n",
      "Partition 23: 2410 videos\n",
      "Partition 24: 2786 videos\n",
      "Partition 25: 2546 videos\n",
      "Partition 26: 2433 videos\n",
      "Partition 27: 2353 videos\n",
      "Partition 28: 2085 videos\n",
      "Partition 29: 2557 videos\n",
      "Partition 30: 2236 videos\n",
      "Partition 31: 2470 videos\n",
      "Partition 32: 2356 videos\n",
      "Partition 33: 2274 videos\n",
      "Partition 34: 2658 videos\n",
      "Partition 35: 2535 videos\n",
      "Partition 36: 2339 videos\n",
      "Partition 37: 2655 videos\n",
      "Partition 38: 2477 videos\n",
      "Partition 39: 2556 videos\n",
      "Partition 40: 2420 videos\n",
      "Partition 41: 2222 videos\n",
      "Partition 42: 2384 videos\n",
      "Partition 43: 2546 videos\n",
      "Partition 44: 2665 videos\n",
      "Partition 45: 2346 videos\n",
      "Partition 46: 2202 videos\n",
      "Partition 47: 2406 videos\n",
      "Partition 48: 2463 videos\n",
      "Partition 49: 3134 videos\n",
      "\n",
      "Total dataset:\n",
      "  107232 training videos\n",
      "  11914 validation videos\n",
      "\n",
      "Creating epoch video-blocks...\n",
      "VideoTuple: DEFAULT, 0, train, gpwnogboea.mp4, 33, FAKE, mgwbjxpbxr.mp4, False\n",
      "CPU times: user 5.3 s, sys: 5.25 s, total: 10.6 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This function reads each partition's metadata.json file, compiles a correpsonding \n",
    "# list of all videos, split into order-randomized training and validation sets,\n",
    "# then assigns these to epoch blocks and inserts everything into the database.\n",
    "\n",
    "def create_videos_data(istart, istop=None, validation_split=0.1, epochsz=200):\n",
    "#{\n",
    "    # Like range args\n",
    "    if istop is None: istart, istop = 0, istart\n",
    "\n",
    "    vtrains, vvalids = [],[]\n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        vpart = []\n",
    "        try:\n",
    "            # Store all valid tuples from the partition metadata file\n",
    "            with open(f\"{df.traindir(i)}/metadata.json\") as jsonfile:\n",
    "            #{\n",
    "                metadata = json.load(jsonfile)\n",
    "                for vidname, meta in metadata.items():\n",
    "                #{                \n",
    "                    if df.file_exists(f\"{df.traindir(i)}/{vidname}\"):\n",
    "                        vtup = pgdb.VideoTuple(vidname=vidname, part_id=i, label=meta['label'])\n",
    "                        if meta['label'] == 'REAL': vpart.append(vtup) \n",
    "                        elif df.file_exists(f\"{df.traindir(i)}/{meta['original']}\"):\n",
    "                            vtup.origname = meta['original']\n",
    "                            vpart.append(vtup)\n",
    "                #}\n",
    "            #}\n",
    "        except PermissionError as err: print(\"ERROR:\", err)\n",
    "\n",
    "        # Randomly select a validation subset\n",
    "        nvalids = round(validation_split*len(vpart))\n",
    "        vindices = set(random.sample(range(len(vpart)), nvalids))\n",
    "\n",
    "        # Separate into respective split\n",
    "        for j in range(len(vpart)):\n",
    "        #{\n",
    "            if j in vindices:\n",
    "                vpart[j].split = \"validate\"\n",
    "                vvalids.append(vpart[j])\n",
    "            else:\n",
    "                vpart[j].split = \"train\"\n",
    "                vtrains.append(vpart[j])\n",
    "        #}\n",
    "        \n",
    "        print(f\"Partition {i}: {len(vpart)} videos\")\n",
    "    #}\n",
    "\n",
    "    print(f\"\\nTotal dataset:\")\n",
    "    print(f\"  {len(vtrains)} training videos\")\n",
    "    print(f\"  {len(vvalids)} validation videos\")\n",
    "    print(\"\\nCreating epoch video-blocks...\")\n",
    "    \n",
    "    # Randomize video orders\n",
    "    np.random.shuffle(vtrains)\n",
    "    np.random.shuffle(vvalids)\n",
    "\n",
    "    # Init step sizes and create epoch blocks\n",
    "    nvalids = int(epochsz*validation_split)    \n",
    "    rgt = range(0, len(vtrains), epochsz - nvalids)\n",
    "    rgv = range(0, len(vvalids), nvalids)\n",
    "    \n",
    "    #eblocks = []\n",
    "    for blkid, (ti, vi) in enumerate(zip(rgt, rgv)):\n",
    "    #{\n",
    "        for vtup in vtrains[ti:ti+epochsz-nvalids]: vtup.blk_id = blkid; print (vtup); return\n",
    "        #eblocks.append(pgdb.EpochTuple(blk_id=blkid, split=\"train\"))\n",
    "\n",
    "        for vtup in vvalids[vi:vi+nvalids]: vtup.blk_id = blkid\n",
    "        #eblocks.append(pgdb.EpochTuple(blk_id=blkid, split=\"validate\"))\n",
    "    #}\n",
    "    print(f\"  {blkid+1} epoch video-blocks created\")\n",
    "\n",
    "    print(f\"\\nClipping nvideos from tail:\")\n",
    "    print(f\"  {len(vtrains[ti:])} training videos\")\n",
    "    print(f\"  {len(vvalids[vi:])} validation videos\")\n",
    "    \n",
    "    del vtrains[ti:]\n",
    "    del vvalids[vi:]\n",
    "    \n",
    "    print(f\"\\nAttempting database insert of:\")\n",
    "    #print(f\"  {len(eblocks)} epoch blocks\")\n",
    "    print(f\"  {len(vtrains)} training videos\")\n",
    "    print(f\"  {len(vvalids)} validation videos\\n\")\n",
    "        \n",
    "    # Insert epoch blocks and videos into DB\n",
    "    with pgdb.PostgreSqlHandle() as db_handle:\n",
    "        if not db_handle.initialize_database(): print(\"Populate aborted\")\n",
    "        else: db_handle.populate_database(vtrains, vvalids)  \n",
    "#}\n",
    "\n",
    "create_videos_data(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "with pgdb.PostgreSqlHandle() as db_handle:\n",
    "    a = db_handle.sqlquery(\"SELECT to_regclass('epoch_queue')\", fetch='one')[0]\n",
    "    print(a is not None, type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing database initialization...\n",
      "ERROR: syntax error at or near \"blk_id\"\n",
      "LINE 3:                 blk_id INTEGER NOT NULL,\n",
      "                        ^\n",
      "\n",
      "Populate aborted\n"
     ]
    }
   ],
   "source": [
    "with pgdb.PostgreSqlHandle() as db_handle:\n",
    "    if not db_handle.initialize_database(): print(\"Populate aborted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
