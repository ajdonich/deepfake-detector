{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time\n",
    "import functools\n",
    "import cv2, json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake Data Preprocessor\n",
    "\n",
    "This Notebook is for preprocessing the training data, i.e. (currently anyway) creating difference-blend-mode image files   \n",
    "for every frame of the faked video, plus a final \"fakerprint\" image (see comments below). You should be able to preprocess  \n",
    "kaggle's little sample dataset in only like 20 minutes. This video player: https://darbyjohnston.github.io/DJV/ can play  \n",
    "the JPG difference-image sequences as if they were a video if you want (Premiere can do this too, but not Quicktime).\n",
    "\n",
    "Make sure to correctly set the **DATA_LAKE** below before preprocessing (and **run_sample_data** flag if apropos). \n",
    "\n",
    "#### Main Function Table of Contents:\n",
    "+ **summarize_data()** - Just has a tertiary look at the dataset and prints out some metrics.\n",
    "+ **faked_video_pairs()** - Creates list of fake/original video pairs from metadata.json files.\n",
    "+ **create_fakerframes_cv2()** - Creates the actual difference and fakerprint image files.\n",
    "+ **preprocess_data()** - Orchestrates calls to faked_video_pairs() and create_fakerframes_cv2() on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: set flag to False to switch to real dataset\n",
    "run_sample_data = True\n",
    "\n",
    "# This is for real dataset. Only DATA_LAKE need adjustment\n",
    "DATA_LAKE = '/Volumes/My Book/deepfake-detect-datalake'\n",
    "\n",
    "# Built off DATA_LAKE and kaggle partition directory structure.\n",
    "DATA_TRAIN_PART = 'dfdc_train_part_IDX'\n",
    "ROOT_DATA_TRAIN = f'{DATA_LAKE}/{DATA_TRAIN_PART}'\n",
    "ROOT_FAKER_FRAMES = f'{DATA_LAKE}/dfdc_frames_part_IDX'\n",
    "\n",
    "def trainpart(i): return DATA_TRAIN_PART.replace('IDX', str(i))\n",
    "def traindir(i): return ROOT_DATA_TRAIN.replace('IDX', str(i))\n",
    "def fakerdir(i): return ROOT_FAKER_FRAMES.replace('IDX', str(i))\n",
    "\n",
    "if run_sample_data:\n",
    "#{\n",
    "    # These should be set right if you create a data directory in\n",
    "    # the repo root and unzip the kaggle sample data file there\n",
    "\n",
    "    DATA_LAKE = '../data'\n",
    "    DATA_TRAIN_PART = 'train_sample_videos'\n",
    "    ROOT_DATA_TRAIN = f'{DATA_LAKE}/{DATA_TRAIN_PART}'\n",
    "    ROOT_FAKER_FRAMES = f'{DATA_LAKE}/train_sample_frames'\n",
    "    \n",
    "    def trainpart(i): return DATA_TRAIN_PART\n",
    "    def traindir(i): return ROOT_DATA_TRAIN\n",
    "    def fakerdir(i): return ROOT_FAKER_FRAMES\n",
    "#}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_exists(filename):\n",
    "    try:\n",
    "        with open(filename) as video: return True\n",
    "    except FileNotFoundError: return False\n",
    "\n",
    "# This fcn takes just an initial glance at the video and metadata\n",
    "# in the training set and prints out some informational metrics.\n",
    "def summarize_data(istart, istop=None, netsum=True, verbose=False):\n",
    "#{\n",
    "    if istop is None:\n",
    "        istart, istop = 0, istart\n",
    "\n",
    "    summation = []\n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        try:\n",
    "        #{\n",
    "            numvids, numtrain, vidsfounds, numfakes, origfound = 0,0,0,0,0\n",
    "            with open(f\"{traindir(i)}/metadata.json\") as jsonfile:\n",
    "            #{\n",
    "                metadata = json.load(jsonfile)\n",
    "                for vidname, meta in metadata.items():\n",
    "                #{\n",
    "                    vname = f\"{traindir(i)}/{vidname}\"\n",
    "                    numvids += 1\n",
    "\n",
    "                    if meta['split'] == 'train': numtrain += 1\n",
    "\n",
    "                    if file_exists(vname): vidsfounds += 1\n",
    "\n",
    "                    if (meta['label'] == 'FAKE'):\n",
    "                        oname = f\"{traindir(i)}/{meta['original']}\"\n",
    "                        if file_exists(oname): origfound += 1\n",
    "                        numfakes += 1\n",
    "                #}\n",
    "            #}\n",
    "        #}\n",
    "        except PermissionError as err: print(\"ERROR:\", err)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Partition: {trainpart(i)}:\")\n",
    "            print(f\"  Numb of primary video meta-references/train split: {numvids}/{numtrain}\")\n",
    "            print(f\"  Numb of actual primary videos files found on disk: {vidsfounds}\")\n",
    "            print(f\"  Numb of fake/original meta-pairs: {numfakes}\")\n",
    "            print(f\"  Numb of actual video pairs found on disk: {origfound}\")\n",
    "            \n",
    "        if (numvids != vidsfounds) or (numvids != numtrain) or (numfakes != origfound):\n",
    "            print(f\"  DISCREPENCY found in partition {trainpart(i)}\")\n",
    "            \n",
    "        if netsum: summation.append((numvids, numtrain, vidsfounds, numfakes, origfound))\n",
    "    #}\n",
    "    \n",
    "    if netsum:\n",
    "    #{\n",
    "        numvids, numtrain, vidsfounds, numfakes, origfound = functools.reduce(\n",
    "            lambda st1, st2: (st1[0]+st2[0], st1[1]+st2[1], st1[2]+st2[2], \n",
    "                              st1[3]+st2[3], st1[4]+st2[4]), summation)\n",
    "        \n",
    "        print(f\"\\nNet dataset summary:\")\n",
    "        print(f\"  Numb of primary video meta-references/train split: {numvids}/{numtrain}\")\n",
    "        print(f\"  Numb of actual primary videos files found on disk: {vidsfounds}\")\n",
    "        print(f\"  Numb of fake/original meta-pairs: {numfakes}\")\n",
    "        print(f\"  Numb of actual video pairs found on disk: {origfound}\\n\")\n",
    "    #}\n",
    "#}\n",
    "\n",
    "if not run_sample_data:\n",
    "#{\n",
    "    summarize_data(50)\n",
    "\n",
    "    # Partitions 18 and 35 appear slightly incomplete. C'est la vie.\n",
    "    # summarize_data(18, 19, netsum=False, verbose=True)\n",
    "    # summarize_data(35, 36, netsum=False, verbose=True)\n",
    "#}\n",
    "else: summarize_data(1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fcn creates an array of the valid deepfake-original video name pairs\n",
    "# based on the metadata.json file and the existence (or not) of the videos\n",
    "def faked_video_pairs(ipart):\n",
    "#{\n",
    "    pc_pairs = []\n",
    "    \n",
    "    try:\n",
    "    #{\n",
    "        with open(f\"{traindir(ipart)}/metadata.json\") as jsonfile:\n",
    "        #{\n",
    "            metadata = json.load(jsonfile)\n",
    "            for vidname, meta in metadata.items():\n",
    "                vname = f\"{traindir(ipart)}/{vidname}\"\n",
    "                if file_exists(vname) and (meta['label'] == 'FAKE'):\n",
    "                    oname = f\"{traindir(ipart)}/{meta['original']}\"\n",
    "                    if file_exists(oname): pc_pairs.append((vname, oname))\n",
    "        #}\n",
    "    #}  \n",
    "    except PermissionError as err: print(\"ERROR:\", err)\n",
    "    return pc_pairs\n",
    "#}\n",
    "\n",
    "# Given a video name pair: where pc_pair[0] is a deepfake, pc_pair[1] its original/parent, \n",
    "# this function creates one \"fakerframe\" JPEG image (a difference blend mode image) per \n",
    "# video frame pair (expect 300 per 10s video at 30fps), and a single 'fakerprint' (a kind \n",
    "# of video fingerprint), which is the scalled sum the fakerframes. Saves files to datapath.\n",
    "def create_fakerframes_cv2(pc_pair, datapath):\n",
    "#{\n",
    "    video = cv2.VideoCapture(pc_pair[0])\n",
    "    orig = cv2.VideoCapture(pc_pair[1])\n",
    "    \n",
    "    count, vsuccess, osuccess, fakerprint = 0, True, True, None\n",
    "    while video.isOpened() and orig.isOpened() and vsuccess and osuccess:\n",
    "    #{\n",
    "        # Each fakerframe created similarly to the Photoshop\n",
    "        # difference blend-mode b/w the faked frame and its original\n",
    "        # https://helpx.adobe.com/photoshop/using/blending-modes.htm \n",
    "        vsuccess, videoframe = video.read()\n",
    "        osuccess, origframe = orig.read()\n",
    "        \n",
    "        if vsuccess and osuccess:           \n",
    "            fakerframe = cv2.subtract(cv2.max(videoframe, origframe), cv2.min(videoframe, origframe))\n",
    "            cv2.imwrite(f\"{datapath}/fakerframe{count}.jpg\", fakerframe)\n",
    "            \n",
    "            if fakerprint is None: fakerprint = fakerframe.astype(np.uint16)\n",
    "            else: fakerprint = fakerprint + fakerframe\n",
    "        \n",
    "        count += 1\n",
    "    #}\n",
    "    \n",
    "    fkmax = np.amax(fakerprint)\n",
    "    fscale = fkmax if fkmax <= 255 else fkmax / 255\n",
    "    fakerprint = np.divide(fakerprint, fscale).astype(np.uint16)\n",
    "    cv2.imwrite(f\"{datapath}/fakerprint.jpg\", fakerprint)\n",
    "    \n",
    "    video.release()\n",
    "    orig.release()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This fcn puts it together, generates video pairs, defines a datapath per,\n",
    "# pair, and generates all the fakerframes and the fakerprint for that pair.\n",
    "def preprocess_data(istart, istop=None, batchsz=10):\n",
    "#{\n",
    "    if istop is None: istart, istop = 0, istart\n",
    "\n",
    "    batchtimes, batchnum = [], 1\n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        parttime = time.time()\n",
    "        fpairs = faked_video_pairs(i)\n",
    "        if not os.path.isdir(fakerdir(i)): os.mkdir(fakerdir(i))\n",
    "        print(f\"Preprocessing {len(fpairs)} video pairs in partition {trainpart(i)}:\")\n",
    "        for j, pc_pair in enumerate(fpairs):\n",
    "        #{\n",
    "            # This is some finagled code to pickup somewhere mid-partition\n",
    "            #if re.split(r'[/.]', pc_pair[0])[-2] == 'wmoigsbnem': pickup = True\n",
    "            #if not pickup: continue\n",
    "        \n",
    "            batchtimes.append(time.time())\n",
    "            datapath = f\"{fakerdir(i)}/{re.split(r'[/.]', pc_pair[0])[-2]}\"\n",
    "            if not os.path.isdir(datapath): os.mkdir(datapath)\n",
    "            create_fakerframes_cv2(pc_pair, datapath)\n",
    "                        \n",
    "            batchtimes[-1] = time.time() - batchtimes[-1]\n",
    "            if len(batchtimes) == batchsz:\n",
    "                print(f\"  Average preprocess time per pair for batch {batchnum}: {np.average(batchtimes):.2f} sec\")\n",
    "                batchnum += 1; batchtimes = []\n",
    "        #}\n",
    "        print(f\"  Total partition {trainpart(i)} preprocessing time: {(time.time()-parttime):.2f} sec\")\n",
    "    #}\n",
    "#}\n",
    "\n",
    "if not run_sample_data:\n",
    "#{\n",
    "    # With arg=1, preprocesses just the first partition (1334 videos). \n",
    "    # On my iMac, this averaged ~28 sec/video (or 10.5 hrs total).\n",
    "    preprocess_data(1)\n",
    "#}\n",
    "else: preprocess_data(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
