{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Camera Display w/OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keystrokes to close display \n",
    "# window: q, esc, or <space bar>\n",
    "\n",
    "KEY_Q     = 113\n",
    "KEY_ESC   = 27\n",
    "KEY_SPACE = 32\n",
    "\n",
    "def cleanup(camera, win_name):\n",
    "    camera.release()\n",
    "    cv2.destroyWindow(win_name)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_camera(win_name='camera'):\n",
    "#{\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow(win_name)\n",
    "    cv2.startWindowThread()\n",
    "\n",
    "    try:\n",
    "    #{\n",
    "        while True:\n",
    "            vsuccess, cameraframe = camera.read()\n",
    "            cv2.imshow(win_name, cameraframe)\n",
    "\n",
    "            k = cv2.waitKey(30)\n",
    "            if k == KEY_Q or k == KEY_ESC or k == KEY_SPACE:\n",
    "                cleanup(camera, win_name)\n",
    "                break\n",
    "    #}  \n",
    "    except KeyboardInterrupt as keyinterrupt:\n",
    "        cleanup(camera, win_name)\n",
    "        raise keyinterrupt # Rethrow to notebook cell\n",
    "#}\n",
    "\n",
    "display_camera()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection Applied to Camera Stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimented with including face profile detection with:\n",
    "# profile_config = '../config/haarcascade_profileface.xml'\n",
    "# profile_detector = cv2.CascadeClassifier(profile_config)\n",
    "\n",
    "# But decided against it for now because the detector was \n",
    "# already causing camera lag, plus it only detects left side \n",
    "# profiles, and would thus require an image flip and a full \n",
    "# second pass through the detector to be fully effective.\n",
    "\n",
    "def face_detect_camera(win_name='camera'):\n",
    "#{    \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow(win_name)\n",
    "    cv2.startWindowThread()\n",
    "\n",
    "    frontal_config = '../config/haarcascade_frontalface_default.xml'\n",
    "    frontal_detector = cv2.CascadeClassifier(frontal_config)\n",
    "\n",
    "    try:\n",
    "    #{\n",
    "        while True:\n",
    "        #{\n",
    "            # Read camera frame\n",
    "            vsuccess, cameraframe = camera.read()\n",
    "            \n",
    "            # Create gray scale version of frame and run classifiers\n",
    "            greyframe = cv2.cvtColor(cameraframe, cv2.COLOR_BGR2GRAY)\n",
    "            facerects = frontal_detector.detectMultiScale(greyframe, 1.1, 4)\n",
    "                                    \n",
    "            # Draw detected rectangles onto frame\n",
    "            for (x, y, w, h) in facerects: cv2.rectangle(\n",
    "                cameraframe, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "            # Display\n",
    "            cv2.imshow(win_name, cameraframe)\n",
    "            k = cv2.waitKey(30)\n",
    "            \n",
    "            # Catch window close keystrokes\n",
    "            if k == KEY_Q or k == KEY_ESC or k == KEY_SPACE:\n",
    "                cleanup(camera, win_name)\n",
    "                break\n",
    "        #}\n",
    "    #}\n",
    "    except KeyboardInterrupt as keyinterrupt:\n",
    "        cleanup(camera, win_name)\n",
    "        raise keyinterrupt # Rethrow to notebook cell\n",
    "#}\n",
    "\n",
    "face_detect_camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect_camera2(win_name='camera'):\n",
    "#{    \n",
    "    camera = cv2.VideoCapture(0)\n",
    "    cv2.namedWindow(win_name)\n",
    "    cv2.startWindowThread()\n",
    "\n",
    "    frontal_config = '../config/haarcascade_frontalface_default.xml'\n",
    "    frontal_detector = cv2.CascadeClassifier(frontal_config)\n",
    "\n",
    "    try:\n",
    "    #{\n",
    "        while True:\n",
    "        #{\n",
    "            # Read camera frame\n",
    "            vsuccess, cameraframe = camera.read()\n",
    "            \n",
    "            # Run classifiers\n",
    "            facerects = face_recognition.face_locations(cameraframe)\n",
    "                                                \n",
    "            # Draw detected rectangles onto frame\n",
    "            for (top, rht, bot, lft) in facerects: cv2.rectangle(\n",
    "                cameraframe, (lft, top), (rht, bot), (255, 0, 0), 2)\n",
    "            \n",
    "            # Display\n",
    "            cv2.imshow(win_name, cameraframe)\n",
    "            k = cv2.waitKey(30)\n",
    "            \n",
    "            # Catch window close keystrokes\n",
    "            if k == KEY_Q or k == KEY_ESC or k == KEY_SPACE:\n",
    "                cleanup(camera, win_name)\n",
    "                break\n",
    "        #}\n",
    "    #}\n",
    "    except KeyboardInterrupt as keyinterrupt:\n",
    "        cleanup(camera, win_name)\n",
    "        raise keyinterrupt # Rethrow to notebook cell\n",
    "#}\n",
    "\n",
    "face_detect_camera2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection Applied to Dataset\n",
    "#### Using OpenCV Haar Cascade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect_dataset_cascade(istart=1, istop=None):\n",
    "#{\n",
    "    # Like range args\n",
    "    if istop is None:\n",
    "        istart, istop = 0, istart\n",
    "\n",
    "    frontal_config = '../config/haarcascade_frontalface_default.xml'\n",
    "    frontal_detector = cv2.CascadeClassifier(frontal_config)\n",
    "        \n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        try:\n",
    "        #{\n",
    "            if not os.path.isdir(df.frectdir(i)): os.mkdir(df.frectdir(i))\n",
    "            for videoname in df.DataSplitter(i, 0.0, 0.25, False).train_split:\n",
    "            #{\n",
    "                datapath = f\"{df.frectdir(i)}/{re.split(r'[/.]', videoname)[-2]}\"\n",
    "                if not os.path.isdir(datapath): os.mkdir(datapath)\n",
    "                    \n",
    "                _face_detect_video(f\"{df.traindir(i)}/{videoname}\", datapath, frontal_detector)\n",
    "            #}\n",
    "        #}\n",
    "        except PermissionError as err: print(\"ERROR:\", err)\n",
    "    #}\n",
    "#}\n",
    "\n",
    "def _face_detect_video(videopath, datapath, fdetector):\n",
    "#{\n",
    "    video = cv2.VideoCapture(videopath)\n",
    "    count, rcount, vsuccess = 0, 0, True\n",
    "    \n",
    "    initial = time.time()\n",
    "    while video.isOpened() and vsuccess:\n",
    "    #{\n",
    "        vsuccess, videoframe = video.read()\n",
    "        \n",
    "        if vsuccess:\n",
    "            # Create gray scale version of frame and run classifiers\n",
    "            greyframe = cv2.cvtColor(videoframe, cv2.COLOR_BGR2GRAY)\n",
    "            facerects = fdetector.detectMultiScale(greyframe, 1.1, 4)\n",
    "                                    \n",
    "            # Draw detected rectangles onto frame\n",
    "            for (x, y, w, h) in facerects: cv2.rectangle(\n",
    "                videoframe, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "            if isinstance(facerects, np.ndarray):\n",
    "                cv2.imwrite(f\"{datapath}/frectframe{count}.jpg\", videoframe)\n",
    "                rcount += 1\n",
    "                \n",
    "        count += 1\n",
    "    #}\n",
    "    \n",
    "    print(f\"{videopath.split(\"/\")[-1]} : numb face frames:{rcount}/{count-1} ({int(rcount/(count-1)*100)}%) in {time.time()-initial:.2f} sec\")\n",
    "#}\n",
    "\n",
    "face_detect_dataset_cascade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using dlib face_detection: https://github.com/ageitgey/face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect_dataset2(istart=1, istop=None):\n",
    "#{\n",
    "    # Like range args\n",
    "    if istop is None:\n",
    "        istart, istop = 0, istart\n",
    "        \n",
    "    for i in range(istart, istop):\n",
    "    #{\n",
    "        try:\n",
    "        #{\n",
    "            if not os.path.isdir(df.frectdir(i)): os.mkdir(df.frectdir(i))\n",
    "            for videoname in df.DataSplitter(i, 0.0, 0.25, False).train_split:\n",
    "            #{\n",
    "                datapath = f\"{df.frectdir(i)}/{re.split(r'[/.]', videoname)[-2]}\"\n",
    "                if not os.path.isdir(datapath): os.mkdir(datapath)\n",
    "                    \n",
    "                rcount = _face_detect_video2(f\"{df.traindir(i)}/{videoname}\", datapath, scale=0.25)\n",
    "                if rcount < 0: _face_detect_video2(f\"{df.traindir(i)}/{videoname}\", datapath, scale=0.5)\n",
    "                \n",
    "            #}\n",
    "        #}\n",
    "        except PermissionError as err: print(\"ERROR:\", err)\n",
    "    #}\n",
    "#}\n",
    "\n",
    "def _face_detect_video2(videopath, datapath, scale=0.25):\n",
    "#{\n",
    "    video = cv2.VideoCapture(videopath)\n",
    "    count, rcount, vsuccess = 0, 0, True\n",
    "    revscale = 1.0/scale\n",
    "    \n",
    "    initial = time.time()\n",
    "    while video.isOpened() and vsuccess:\n",
    "    #{\n",
    "        vsuccess, videoframe = video.read()\n",
    "        \n",
    "        if vsuccess:\n",
    "            # Scale down by 1/4, convert from BGR to RGB, run face locator\n",
    "            smallrgbframe = cv2.resize(videoframe, (0, 0), fx=scale, fy=scale)[:, :, ::-1]\n",
    "            facerects = face_recognition.face_locations(smallrgbframe)\n",
    "            \n",
    "            # Draw detected (scaled back up) rectangles onto frame\n",
    "            for (top, rht, bot, lft) in map(functools.partial(_revscale, rscale=revscale), facerects):\n",
    "                cv2.rectangle(videoframe, (lft, top), (rht, bot), (255, 0, 0), 2)\n",
    "                        \n",
    "            if facerects:\n",
    "                cv2.imwrite(f\"{datapath}/frectframe{count}.jpg\", videoframe)\n",
    "                rcount += 1\n",
    "                \n",
    "        count += 1\n",
    "    #}\n",
    "    \n",
    "    vidirname = re.split(\"[/\\.]\", videopath)[-2]\n",
    "    print(f\"{vidirname} : numb face frames:{rcount}/{count-1} in {time.time()-initial:.2f} sec\")\n",
    "    return rcount\n",
    "#}\n",
    "\n",
    "def _revscale(quad, rscale): return [int(val*rscale) for val in quad]\n",
    "\n",
    "face_detect_dataset2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
